#!/usr/bin/env python3
"""
Dead Code Removal Script - Phase 2

This script identifies and removes/archives unused code modules and 
duplicate utilities identified in the comprehensive analysis.

TARGETS FOR REMOVAL/ARCHIVAL:
1. Unused ML modules (ml/, machine_learning/)
2. Empty/incomplete strategies (strategies/)  
3. Incomplete validation modules (validation/)
4. Duplicate directories (backtest/ vs backtesting/, analysis/ vs analytics/)
5. Redundant risk management systems (now that we have unified)
6. Duplicate utilities across modules

APPROACH:
- Archive instead of delete (for safety)
- Create usage analysis to confirm modules are unused
- Remove duplicate directories
- Consolidate remaining functionality
"""

import os
import shutil
import ast
import re
from pathlib import Path
from typing import Dict, List, Set, Tuple
from dataclasses import dataclass

@dataclass
class ModuleAnalysis:
    """Analysis of a module's usage and dependencies"""
    path: str
    lines_of_code: int
    classes: List[str]
    functions: List[str]
    imports_from: List[str]  # What this module imports
    imported_by: List[str]   # What modules import this
    is_used: bool
    usage_score: float  # 0-100, higher = more used

class DeadCodeRemover:
    """Removes unused and duplicate code from the repository"""
    
    def __init__(self, repo_root: str):
        self.repo_root = Path(repo_root)
        self.src_root = self.repo_root / 'src' / 'bot'
        
        # Archive directory for removed code
        self.archive_dir = self.repo_root / 'archive' / 'removed_code'
        self.archive_dir.mkdir(parents=True, exist_ok=True)
        
        # Modules targeted for removal/archival
        self.removal_targets = {
            'unused_ml': [
                'ml',  # Unused ML engine
                'machine_learning'  # Duplicate ML
            ],
            'incomplete_modules': [
                'strategies',  # Empty strategies
                'validation'   # Incomplete validation
            ],
            'duplicate_directories': [
                'backtest',    # Duplicate of backtesting
                'analysis'     # Duplicate of analytics  
            ],
            'redundant_risk': [
                'risk_management',  # Now consolidated
                'dynamic_risk'      # Now consolidated
            ]
        }
        
        # Track all python files for import analysis
        self.all_python_files = []
        self.module_analyses = {}
        
    def analyze_codebase(self) -> Dict[str, ModuleAnalysis]:
        """Analyze the entire codebase for usage patterns"""
        print("üîç Analyzing codebase for dead code...")
        
        # Find all Python files
        self.all_python_files = list(self.src_root.rglob("*.py"))
        
        # Analyze each module
        for py_file in self.all_python_files:
            if '__pycache__' in str(py_file):
                continue
                
            analysis = self._analyze_file(py_file)
            self.module_analyses[str(py_file)] = analysis
            
        # Calculate usage relationships
        self._calculate_usage_relationships()
        
        return self.module_analyses
    
    def _analyze_file(self, file_path: Path) -> ModuleAnalysis:
        """Analyze a single Python file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            lines_of_code = len([line for line in content.splitlines() 
                               if line.strip() and not line.strip().startswith('#')])
            
            classes = []
            functions = []
            imports_from = []
            
            # Parse AST
            try:
                tree = ast.parse(content)
                for node in ast.walk(tree):
                    if isinstance(node, ast.ClassDef):
                        classes.append(node.name)
                    elif isinstance(node, ast.FunctionDef):
                        functions.append(node.name)
                    elif isinstance(node, ast.Import):
                        for alias in node.names:
                            imports_from.append(alias.name)
                    elif isinstance(node, ast.ImportFrom):
                        if node.module:
                            imports_from.append(node.module)
            except SyntaxError:
                pass
                
            return ModuleAnalysis(
                path=str(file_path),
                lines_of_code=lines_of_code,
                classes=classes,
                functions=functions,
                imports_from=imports_from,
                imported_by=[],  # Will be filled later
                is_used=False,   # Will be calculated later
                usage_score=0.0  # Will be calculated later
            )
            
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error analyzing {file_path}: {e}")
            return ModuleAnalysis(
                path=str(file_path),
                lines_of_code=0,
                classes=[],
                functions=[],
                imports_from=[],
                imported_by=[],
                is_used=False,
                usage_score=0.0
            )
    
    def _calculate_usage_relationships(self):
        """Calculate which modules import which other modules"""
        print("üîó Calculating module dependencies...")
        
        for file_path, analysis in self.module_analyses.items():
            file_module = self._path_to_module_name(file_path)
            
            # Find what imports this module
            for other_path, other_analysis in self.module_analyses.items():
                if other_path == file_path:
                    continue
                    
                # Check if other module imports this one
                for import_name in other_analysis.imports_from:
                    if self._imports_match(import_name, file_module):
                        analysis.imported_by.append(other_path)
        
        # Calculate usage scores
        for analysis in self.module_analyses.values():
            analysis.usage_score = self._calculate_usage_score(analysis)
            analysis.is_used = analysis.usage_score > 10  # Threshold for "used"
    
    def _path_to_module_name(self, file_path: str) -> str:
        """Convert file path to module name"""
        path = Path(file_path)
        relative_path = path.relative_to(self.src_root)
        
        # Remove .py extension and convert to module notation
        module_parts = list(relative_path.parts[:-1])  # Remove filename
        if relative_path.stem != '__init__':
            module_parts.append(relative_path.stem)
            
        return 'bot.' + '.'.join(module_parts)
    
    def _imports_match(self, import_name: str, module_name: str) -> bool:
        """Check if an import statement matches a module"""
        return (import_name in module_name or 
                module_name.endswith(import_name) or
                import_name.endswith(module_name.split('.')[-1]))
    
    def _calculate_usage_score(self, analysis: ModuleAnalysis) -> float:
        """Calculate usage score for a module (0-100)"""
        score = 0.0
        
        # Base score from imports
        score += len(analysis.imported_by) * 20  # 20 points per importing module
        
        # Bonus for having classes/functions (indicates functionality)
        score += len(analysis.classes) * 5
        score += len(analysis.functions) * 2
        
        # Bonus for lines of code (more code = more likely to be important)
        score += min(analysis.lines_of_code / 10, 20)  # Cap at 20 points
        
        return min(score, 100)  # Cap at 100
    
    def identify_dead_code(self) -> Dict[str, List[str]]:
        """Identify dead code based on analysis"""
        dead_code = {
            'unused_modules': [],
            'low_usage_modules': [],
            'empty_modules': [],
            'duplicate_modules': []
        }
        
        for path, analysis in self.module_analyses.items():
            path_obj = Path(path)
            relative_path = path_obj.relative_to(self.src_root)
            module_dir = relative_path.parts[0] if relative_path.parts else ''
            
            # Check if in removal targets
            for category, targets in self.removal_targets.items():
                if module_dir in targets:
                    if category == 'unused_ml':
                        dead_code['unused_modules'].append(path)
                    elif category == 'incomplete_modules':
                        dead_code['unused_modules'].append(path)
                    elif category == 'duplicate_directories':
                        dead_code['duplicate_modules'].append(path)
                    elif category == 'redundant_risk':
                        dead_code['duplicate_modules'].append(path)
                    continue
            
            # Additional analysis-based categorization
            if not analysis.is_used and analysis.usage_score < 5:
                dead_code['unused_modules'].append(path)
            elif analysis.usage_score < 20:
                dead_code['low_usage_modules'].append(path)
            elif analysis.lines_of_code < 10:
                dead_code['empty_modules'].append(path)
        
        return dead_code
    
    def archive_dead_code(self, dead_code: Dict[str, List[str]]) -> Dict[str, int]:
        """Archive dead code modules"""
        archived_stats = {
            'modules_archived': 0,
            'files_archived': 0,
            'lines_removed': 0
        }
        
        print("üì¶ Archiving dead code...")
        
        # Group by directory for efficient archival
        directories_to_archive = set()
        
        for category, files in dead_code.items():
            print(f"\nüìÅ {category.upper()}:")
            
            for file_path in files:
                path_obj = Path(file_path)
                relative_path = path_obj.relative_to(self.src_root)
                module_dir = self.src_root / relative_path.parts[0]
                
                if module_dir.exists() and str(module_dir) not in directories_to_archive:
                    directories_to_archive.add(str(module_dir))
                    
                    # Archive the entire directory
                    archive_dest = self.archive_dir / relative_path.parts[0]
                    
                    try:
                        if archive_dest.exists():
                            shutil.rmtree(archive_dest)
                        
                        shutil.copytree(module_dir, archive_dest)
                        
                        # Count files and lines
                        files_count = len(list(archive_dest.rglob("*.py")))
                        lines_count = sum(
                            len(f.read_text(encoding='utf-8').splitlines())
                            for f in archive_dest.rglob("*.py")
                        )
                        
                        print(f"  ‚úÖ {relative_path.parts[0]}/ ‚Üí archive/ "
                              f"({files_count} files, {lines_count} lines)")
                        
                        archived_stats['modules_archived'] += 1
                        archived_stats['files_archived'] += files_count
                        archived_stats['lines_removed'] += lines_count
                        
                    except Exception as e:
                        print(f"  ‚ùå Failed to archive {module_dir}: {e}")
        
        return archived_stats
    
    def remove_archived_directories(self, dead_code: Dict[str, List[str]]) -> Dict[str, int]:
        """Remove directories that have been archived"""
        removal_stats = {
            'directories_removed': 0,
            'files_removed': 0
        }
        
        print("\nüóëÔ∏è  Removing archived directories...")
        
        directories_to_remove = set()
        
        for category, files in dead_code.items():
            for file_path in files:
                path_obj = Path(file_path)
                relative_path = path_obj.relative_to(self.src_root)
                module_dir = self.src_root / relative_path.parts[0]
                
                if str(module_dir) not in directories_to_remove:
                    directories_to_remove.add(str(module_dir))
        
        for dir_path in directories_to_remove:
            dir_path_obj = Path(dir_path)
            
            if dir_path_obj.exists():
                try:
                    files_count = len(list(dir_path_obj.rglob("*")))
                    shutil.rmtree(dir_path_obj)
                    
                    print(f"  ‚úÖ Removed {dir_path_obj.name}/ ({files_count} files)")
                    
                    removal_stats['directories_removed'] += 1
                    removal_stats['files_removed'] += files_count
                    
                except Exception as e:
                    print(f"  ‚ùå Failed to remove {dir_path_obj}: {e}")
        
        return removal_stats
    
    def generate_removal_report(self, dead_code: Dict[str, List[str]], 
                              archived_stats: Dict[str, int],
                              removal_stats: Dict[str, int]) -> str:
        """Generate comprehensive removal report"""
        
        report = f"""
# Dead Code Removal Report - Phase 2

## Summary
- **Modules archived**: {archived_stats['modules_archived']}
- **Files archived**: {archived_stats['files_archived']}
- **Lines removed**: {archived_stats['lines_removed']:,}
- **Directories removed**: {removal_stats['directories_removed']}
- **Total files removed**: {removal_stats['files_removed']}

## Modules Removed by Category

### Unused ML Modules
These were complete ML implementations but not integrated with the main trading system:
"""
        
        for category, files in dead_code.items():
            if files:
                report += f"\n### {category.replace('_', ' ').title()}\n"
                
                # Group by directory
                dirs = {}
                for file_path in files:
                    path_obj = Path(file_path)
                    relative_path = path_obj.relative_to(self.src_root)
                    dir_name = relative_path.parts[0]
                    
                    if dir_name not in dirs:
                        dirs[dir_name] = []
                    dirs[dir_name].append(str(relative_path))
                
                for dir_name, dir_files in dirs.items():
                    report += f"- **{dir_name}/**: {len(dir_files)} files\\n"
                    analysis = self.module_analyses.get(str(self.src_root / dir_files[0]), None)
                    if analysis:
                        report += f"  - Usage score: {analysis.usage_score:.1f}/100\\n"
                        report += f"  - Lines of code: {analysis.lines_of_code:,}\\n"
        
        report += f"""

## Archive Location
All removed code has been archived to: `{self.archive_dir}`

## Impact Analysis
- **Code reduction**: {archived_stats['lines_removed']:,} lines removed
- **Maintenance reduction**: {archived_stats['modules_archived']} fewer modules to maintain
- **Import cleanup**: Simplified dependency graph
- **Build performance**: Faster imports and reduced memory usage

## Safety Measures
- All code archived (not deleted) for potential future recovery
- Full git history preserved
- Usage analysis performed before removal
- Conservative removal approach (only clear dead code)
"""
        
        return report
    
    def execute_removal(self) -> bool:
        """Execute the complete dead code removal process"""
        try:
            print("üöÄ Starting Dead Code Removal - Phase 2...")
            print("=" * 60)
            
            # Step 1: Analyze codebase
            analyses = self.analyze_codebase()
            print(f"üìä Analyzed {len(analyses)} Python files")
            
            # Step 2: Identify dead code
            dead_code = self.identify_dead_code()
            
            total_files = sum(len(files) for files in dead_code.values())
            print(f"üíÄ Identified {total_files} files as dead code")
            
            if total_files == 0:
                print("‚ú® No dead code found - codebase is clean!")
                return True
            
            # Step 3: Archive dead code
            archived_stats = self.archive_dead_code(dead_code)
            
            # Step 4: Remove archived directories
            removal_stats = self.remove_archived_directories(dead_code)
            
            # Step 5: Generate report
            report = self.generate_removal_report(dead_code, archived_stats, removal_stats)
            
            report_path = self.repo_root / 'DEAD_CODE_REMOVAL_REPORT.md'
            with open(report_path, 'w') as f:
                f.write(report)
            
            print(f"\n‚úÖ Dead Code Removal Complete!")
            print(f"üìä Report saved to: {report_path}")
            print(f"üì¶ Archived code at: {self.archive_dir}")
            print(f"üóëÔ∏è  Removed {removal_stats['directories_removed']} directories")
            print(f"üìâ Reduced codebase by {archived_stats['lines_removed']:,} lines")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Dead code removal failed: {e}")
            return False

def main():
    """Main entry point for dead code removal"""
    repo_root = os.getcwd()
    
    print("üßπ Dead Code Removal - Phase 2")
    print("=" * 40)
    
    remover = DeadCodeRemover(repo_root)
    success = remover.execute_removal()
    
    if success:
        print("\nüéâ Phase 2 Complete: Dead Code Removed!")
        print("The codebase is now significantly cleaner and more maintainable.")
    else:
        print("\nüí• Phase 2 failed. Check the errors above.")

if __name__ == "__main__":
    main()